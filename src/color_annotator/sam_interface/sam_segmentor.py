# sam_interface/sam_segmentor.py

import numpy as np
import torch
import cv2
from segment_anything import sam_model_registry, SamPredictor
from pathlib import Path

class SAMSegmentor:
    def __init__(self, model_type="vit_b", checkpoint_path=None):
        if checkpoint_path is None:
            # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•
            current_dir = Path(__file__).resolve().parent.parent
            checkpoint_path = str(current_dir / "checkpoints" / "sam_vit_b.pth")
            print(f"[SAM] ä½¿ç”¨é»˜è®¤æ¨¡å‹è·¯å¾„: {checkpoint_path}")
        
        try:
            print(f"[SAM] åŠ è½½æ¨¡å‹: {model_type} from {checkpoint_path}")
            self.sam = sam_model_registry[model_type](checkpoint=checkpoint_path)
            self.sam.to(device=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
            print("[SAM] æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"[SAM] åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            raise

        self.predictor = SamPredictor(self.sam)

        # åˆå§‹åŒ–å±æ€§
        self.original_shape = None
        self.resized_image = None
        self.scale_factor = 1.0

    def set_image(self, image: np.ndarray, max_size=1024):
        """è®¾ç½®å›¾åƒç»™ SAMï¼Œå¹¶è‡ªåŠ¨ resize åˆ°åˆé€‚å¤§å°"""
        try:
            if image is None:
                raise ValueError("è¾“å…¥å›¾åƒä¸ºç©º")

            if len(image.shape) != 3:
                raise ValueError(f"å›¾åƒç»´åº¦ä¸æ­£ç¡®ï¼ŒæœŸæœ›3ç»´ï¼Œå®é™…{len(image.shape)}ç»´")

            print(f"[SAM] åŸå§‹å›¾åƒå°ºå¯¸: {image.shape}")
            self.original_shape = image.shape[:2]  # (h, w)
            h, w = self.original_shape

            if max(h, w) <= max_size:
                self.resized_image = image
                self.scale_factor = 1.0
                print("[SAM] å›¾åƒå°ºå¯¸åˆé€‚ï¼Œæ— éœ€ç¼©æ”¾")
            else:
                scale = max_size / max(h, w)
                new_w = int(w * scale)
                new_h = int(h * scale)
                self.resized_image = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
                self.scale_factor = 1.0 / scale  # æ³¨æ„è¿™é‡Œæ˜¯ï¼šåŸå›¾åæ ‡ = ç¼©æ”¾ååæ ‡ * scale_factor
                print(f"[SAM] å›¾åƒå·²ç¼©æ”¾è‡³ {new_w}x{new_h}ï¼Œç¼©æ”¾å› å­: {scale:.2f}")

            # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
            if len(self.resized_image.shape) == 2:
                self.resized_image = cv2.cvtColor(self.resized_image, cv2.COLOR_GRAY2RGB)
            elif self.resized_image.shape[2] == 4:
                self.resized_image = self.resized_image[:, :, :3]

            print("[SAM] è®¾ç½®å›¾åƒåˆ°é¢„æµ‹å™¨...")
            self.predictor.set_image(self.resized_image)
            print("[SAM] å›¾åƒè®¾ç½®å®Œæˆ")

        except Exception as e:
            print(f"[SAM] è®¾ç½®å›¾åƒæ—¶å‡ºé”™: {str(e)}")
            raise

    def predict_from_point(self, point: tuple):
        """å¤„ç†å•ç‚¹"""
        scaled_point = (point[0] / self.scale_factor, point[1] / self.scale_factor)
        input_point = np.array([scaled_point])
        input_label = np.array([1])
        masks, scores, logits = self.predictor.predict(
            point_coords=input_point,
            point_labels=input_label,
            multimask_output=False
        )
        return self._resize_mask_back(masks[0])

    def predict_from_points(self, points: np.ndarray, labels: np.ndarray, multimask_output=False):
        """å¤„ç†å¤šç‚¹"""
        # ğŸ’¡ ç‚¹ä¹Ÿè¦ç¼©æ”¾åˆ° resized å°ºå¯¸
        scaled_points = points / self.scale_factor

        masks, scores, logits = self.predictor.predict(
            point_coords=scaled_points,
            point_labels=labels,
            multimask_output=multimask_output
        )
        return self._resize_mask_back(masks[0])

    def _resize_mask_back(self, mask):
        """å°†åˆ†å‰²ç»“æœ resize å›åŸå›¾å¤§å°"""
        if self.scale_factor != 1.0:
            h, w = self.original_shape
            mask = cv2.resize(mask.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)
            return mask.astype(bool)
        return mask

    def segment_with_points(self, image, input_points, input_labels):
        """ä½¿ç”¨ç‚¹æç¤ºè¿›è¡Œåˆ†å‰²"""
        try:
            # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
            if len(image.shape) == 2:
                image = np.stack([image] * 3, axis=-1)
            elif image.shape[2] == 4:
                image = image[:, :, :3]
            
            # è½¬æ¢ä¸ºtorchå¼ é‡
            image_tensor = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
            input_points_tensor = torch.from_numpy(input_points).unsqueeze(0)
            input_labels_tensor = torch.from_numpy(input_labels).unsqueeze(0)
            
            # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            device = next(self.sam.parameters()).device
            image_tensor = image_tensor.to(device)
            input_points_tensor = input_points_tensor.to(device)
            input_labels_tensor = input_labels_tensor.to(device)
            
            # è¿›è¡Œåˆ†å‰²
            with torch.no_grad():
                image_embedding = self.sam.image_encoder(image_tensor.unsqueeze(0))
                sparse_embeddings, dense_embeddings = self.sam.prompt_encoder(
                    points=(input_points_tensor, input_labels_tensor),
                    boxes=None,
                    masks=None,
                )
                mask_predictions, _ = self.sam.mask_decoder(
                    image_embeddings=image_embedding,
                    image_pe=self.sam.prompt_encoder.get_dense_pe(),
                    sparse_prompt_embeddings=sparse_embeddings,
                    dense_prompt_embeddings=dense_embeddings,
                    multimask_output=False,
                )
            
            # å¤„ç†é¢„æµ‹ç»“æœ
            mask_predictions = torch.sigmoid(mask_predictions)
            mask_predictions = mask_predictions > 0.5
            mask = mask_predictions[0, 0].cpu().numpy()
            
            return mask
            
        except Exception as e:
            print(f"[SAM] åˆ†å‰²å¤±è´¥: {str(e)}")
            raise
